# 算法设计与智能计算课程大作业

## 摘要（Abstract）

扩散模型（Diffusion Models）近年来在生成建模领域受到广泛关注，其通过正向逐步加噪与反向逐步去噪的随机过程，实现了从简单分布到复杂数据分布的稳定映射，在图像生成等任务中展现出良好的训练稳定性与生成质量。Denoising Diffusion Probabilistic Models（DDPM）作为扩散模型的代表方法，在理论推导与工程实现之间具有清晰对应关系，适合作为生成模型算法设计与复现的研究对象。

本项目以 **DDPM 原论文为核心研究对象，目标是在 PyTorch 框架下完成 DDPM 的算法级复现**。具体而言，项目系统学习并梳理了 DDPM 的数学原理，在此基础上从零实现了其前向扩散过程、反向去噪采样算法以及基于 U-Net 的噪声预测网络，未直接调用现成的 DDPM 算法库。进一步地，引入 OpenAI 提出的 Improved DDPM 作为拓展方法，对比分析算法改进对生成性能的影响。

实验在 CIFAR-10 数据集上进行。结果表明，所复现的 DDPM 算法能够稳定生成具有合理结构的图像，其生成效果与公开实现基本一致；对比实验显示，Improved DDPM 在生成质量与训练稳定性方面相较原始 DDPM 具有一定提升。综上，本项目验证了 DDPM 算法在 PyTorch 框架下的可复现性，并分析了改进方法带来的性能变化与实现代价，为进一步研究扩散模型提供了实践依据。



## 第一章 引言（Introduction）

### 1.1 研究背景

​	生成模型是人工智能与智能计算领域中的重要研究方向，其核心目标在于**学习真实数据的潜在分布，并从中生成具有统计一致性的新样本**。生成模型不仅在图像、语音与文本生成等感知任务中发挥关键作用，也在数据增强、表示学习、不确定性建模以及科学计算等领域具有重要应用价值。因此，如何设计稳定、高质量且具有可解释性的生成模型，一直是机器学习研究中的核心问题之一。

​	早期具有代表性的生成模型主要包括变分自编码器（Variational Autoencoder, VAE）与生成对抗网络（Generative Adversarial Network, GAN）。VAE 通过显式构建潜变量模型，将数据映射到连续潜在空间，并利用变分推断实现端到端训练，其优点在于理论结构清晰、训练过程稳定，但生成样本往往存在模糊、细节不足的问题。GAN 则通过生成器与判别器之间的对抗博弈进行训练，在图像生成质量上取得了显著突破，但其训练过程高度不稳定，易出现模式崩塌等问题，对超参数和训练技巧较为敏感。上述两类模型在实践中各具优势，但也都面临着稳定性、可控性或理论一致性方面的局限。

​                     <img src="https://www.researchgate.net/publication/356809414/figure/fig1/AS%3A1098577317244930%401638932651307/Example-of-GAN-Architecture.ppm?utm_source=chatgpt.com" alt="Example of GAN Architecture | Download Scientific Diagram" style="zoom:25%;" />                            <img src="https://www.researchgate.net/publication/351917272/figure/fig1/AS%3A1028126347898895%401622135831968/Graphical-representation-of-the-Vanilla-VAE-architecture-The-yellow-orange-and-green.png?utm_source=chatgpt.com" alt="Graphical representation of the Vanilla VAE architecture. The ..." style="zoom:25%;" />

​	从算法思想上看，扩散模型的核心可以概括为“**正向加噪、反向去噪**”：正向过程无需学习参数，仅依赖预先设定的噪声调度策略；反向过程则通过神经网络近似每一步的去噪方向，从而逐步将噪声样本转化为真实数据样本。这种将生成建模问题转化为一系列条件去噪问题的方式，使得扩散模型在理论建模与工程实现之间建立了清晰联系，也为后续算法分析与复现提供了良好的基础。

​		<img src="https://www.researchgate.net/publication/382128283/figure/fig1/AS%3A11431281260181040%401720925618217/The-forward-and-backward-processes-of-the-diffusion-model-The-credit-of-the-used-images.ppm?utm_source=chatgpt.com" alt="https://www.researchgate.net/publication/382128283/figure/fig1/AS%3A11431281260181040%401720925618217/The-forward-and-backward-processes-of-the-diffusion-model-The-credit-of-the-used-images.ppm?utm_source=chatgpt.com" style="zoom:67%;" />

​	正是在这一背景下，DDPM 作为扩散模型中最具代表性的算法之一，为系统理解扩散模型的生成机制与算法设计思想提供了理想切入点。

<img src="https://www.researchgate.net/publication/371956025/figure/fig2/AS%3A11431281250006332%401717736524921/The-schematic-diagram-of-generative-learning-model-includes-VAE-GAN-flow-based-model.tif?utm_source=chatgpt.com" alt="The schematic diagram of generative learning model includes ..." style="zoom:33%;" />

### 1.2 问题定义与研究动机

​	生成建模的核心问题可以表述为：**在仅给定有限样本的情况下，学习一个高维随机变量的真实概率分布，并能够从该分布中高质量地生成新样本**。在概率建模框架下，该问题通常被形式化为对未知数据分布$p_{\text{data}}(x)$的近似建模，其中 $x \in \mathbb{R}^d$ 表示高维观测数据（如图像、语音或信号）。生成模型的目标是构建一个参数化分布 $p_\theta(x)$，使其在统计意义上逼近真实数据分布，即
$$
p_\theta(x) \approx p_{\text{data}}(x)
$$
​	并能够通过从 $p_\theta(x)$ 中采样生成新的、未在训练集中出现的样本。

​	从学习目标上看，生成建模不仅要求模型具备生成能力，即能够产生视觉或语义上合理的新样本，还强调对数据分布结构本身的刻画能力。例如，在图像生成任务中，模型需要捕捉数据在像素空间中的高维相关性；在语音或时序信号建模中，模型还需反映数据在时间维度上的统计规律。因此，生成模型通常被视为一种**显式或隐式的概率密度估计方法**，其研究内容横跨统计建模、随机过程与深度学习等多个领域。

​	本项目关注的问题是如何将 DDPM 原论文中的数学公式完整转化为可运行算法，以及算法在不同深度学习框架下的实现一致性问题。选择 DDPM 作为本项目的研究对象主要出于以下考量：DDPM 的算法结构相对清晰，前向扩散与反向生成过程在数学与实现层面均具有明确划分；另一方面，其理论推导完整，从概率建模到训练目标均有严格的推理基础；同时，DDPM 在实现过程中涉及随机过程建模、神经网络设计与数值算法等多个层面的内容，具有适中的算法设计难度。

### 1.3 项目目标与主要工作

- 理解 DDPM 的数学原理与算法流程
- 基于 PyTorch 完成 DDPM 原始算法的完整复现
- 在 CIFAR-10 数据集上进行实验验证
- 以 OpenAI Improved DDPM 作为拓展方法进行性能对比



## 第二章 DDPM 理论基础与论文算法解析

（Theoretical Background and Algorithm Analysis）

## 2.1 生成建模的概率视角

### 2.1.1 生成模型的统一建模目标

​	生成模型（Generative Models）的核心目标在于**刻画真实数据的生成机制**。设真实数据样本来自未知的数据分布$p_{\text{data}}(x)$, 其中 $x \in \mathbb{R}^d$ 表示高维观测变量（如图像像素、语音信号或其他连续/离散特征）。生成模型通过引入参数化分布 $p_\theta(x)$，试图在统计意义上逼近真实分布，即$p_\theta(x) \approx p_{\text{data}}(x)$。

​	与判别模型侧重于学习条件分布 $p(y\mid x)$ 不同，生成模型直接对数据本身的分布进行建模，其最终目标并非预测标签，而是**从学习到的分布中进行采样**。因此，“生成”在严格意义上可以被理解为：在模型训练完成后，通过从 $p_\theta(x)$ 中采样，得到一组与真实数据在统计特性上一致的样本。

<img src="/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222145532080.png" alt="image-20251222145532080" style="zoom: 25%;" /> <img src="/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222145545726.png" alt="image-20251222145545726" style="zoom:25%;" />

​	生成模型可以根据是否显式建模概率密度，分为**显式密度模型（explicit density models）**与**隐式生成模型（implicit generative models）**两大类。显式密度模型直接给出 $p_\theta(x)$ 或其可计算的下界，例如自回归模型、变分自编码器以及扩散模型等，它们通常具备清晰的概率语义，便于从理论上分析模型行为；而隐式生成模型则不显式给出数据分布的解析形式，而是通过采样过程间接定义生成分布，典型代表为生成对抗网络（GAN）。这两类方法在建模能力、训练稳定性以及理论可解释性方面各具特点，也推动了生成建模方法的不断演进。

### 2.1.2 最大似然估计与 KL 散度

- 所有影响生成模型的共同目标
- 最大化对数似然目标：[\max_\theta \ \mathbb{E}*{x \sim p*{\text{data}}}[\log p_\theta(x)]]与最小化[KL(p_{\text{data}} ,|, p_\theta)]的等价关系
- 生成模型训练的统计意义

---

尽管不同生成模型在结构设计与训练方式上存在显著差异，但从概率建模的角度看，**几乎所有生成模型都围绕着同一个核心目标展开**：使模型分布 $p_\theta(x)$ 在统计意义上尽可能接近真实数据分布 $p_{\text{data}}(x)$。

在显式概率建模框架下，这一目标通常通过**最大似然估计（Maximum Likelihood Estimation, MLE）**来实现。给定从真实数据分布中独立同分布采样得到的数据集，生成模型的训练可以形式化为最大化模型在真实数据上的对数似然期望：
$$
\max_\theta \ \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]
$$
该目标函数具有清晰的统计学含义：它要求模型为真实数据分配尽可能高的概率质量，从而使生成分布在整体上逼近数据分布。

​	进一步地，可以从信息论的角度理解这一优化目标。注意到真实数据分布与模型分布之间的 Kullback–Leibler（KL）散度定义为
$$
KL(p_{\text{data}} \,\|\, p_\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\!\left[\log \frac{p_{\text{data}}(x)}{p_\theta(x)}\right].
$$


由于 $p_{\text{data}}(x)$ 与模型参数 $\theta$ 无关，最小化上述 KL 散度等价于最大化 $\mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]$。因此，**最大化对数似然与最小化 $KL(p_{\text{data}} \,\|\, p_\theta)$ 在优化意义上是等价的**。这一等价关系揭示了生成模型训练的本质：通过调整模型参数，使模型分布在信息论意义上尽可能接近真实数据分布。

### 2.2.1 扩散模型的核心思想

​	扩散模型的核心思想在于通过引入一个逐步扰动的随机过程，将复杂的数据分布转化为易于建模的简单分布。具体而言，扩散模型构造了一条由真实数据样本 $x_0$ 出发的随机演化路径
$$
x_0 \rightarrow x_1 \rightarrow \cdots \rightarrow x_T,
$$
在该过程中，噪声被逐步注入数据，使得样本分布随时间演化并逐渐丧失原有结构。当时间步 T 足够大时，数据分布可以近似转化为各向同性的高斯分布。通过这一设计，扩散模型将原本直接建模复杂高维分布的困难问题，转化为在已知噪声机制下学习逆向去噪过程的问题，为后续稳定而可控的生成建模提供了基础。



## 2.5 训练目标推导：从变分下界到 DDPM 损失函数

### 2.5.1 DDPM 的对数似然下界

​	生成建模的基本目标是最大化数据在模型下的对数似然 $\log p_\theta(x_0)$。然而在扩散模型中，$p_\theta(x_0)$ 并不直接以封闭形式给出，而是通过一条从噪声到数据的**反向马尔可夫链**间接定义。为此，DDPM 将生成过程写成对潜变量序列 $x_{1:T}$ 的边缘化：
$$
p_\theta(x_0)=\int p_\theta(x_{0:T})\,dx_{1:T},\qquad  p_\theta(x_{0:T})=p(x_T)\prod_{t=1}^{T}p_\theta(x_{t-1}\mid x_t),
$$
其中先验 $p(x_T)$ 通常取标准高斯 $\mathcal N(0,I)$。与此同时，正向扩散过程 $q(\cdot)$ 作为一个**已知且固定**的马尔可夫链：
$$
q(x_{1:T}\mid x_0)=\prod_{t=1}^{T}q(x_t\mid x_{t-1})
$$
它刻画了从数据 $x_0$ 出发逐步加噪得到 $x_T$ 的机制。由于直接最大化 $\log p_\theta(x_0)$ 仍然困难，DDPM 引入正向链 $q(x_{1:T}\mid x_0)$ 作为变分分布，构造标准的变分下界（ELBO）：
$$
\log p_\theta(x_0) =\log \int q(x_{1:T}\mid x_0)\frac{p_\theta(x_{0:T})}{q(x_{1:T}\mid x_0)}\,dx_{1:T} \ \ge\  \mathbb E_{q(x_{1:T}\mid x_0)}\!\left[\log \frac{p_\theta(x_{0:T})}{q(x_{1:T}\mid x_0)}\right].
$$
因此，最大化对数似然可被替换为最大化一个可计算的下界目标，这也是 DDPM 训练目标推导的起点。



### 2.5.2 变分下界的分解形式

​	将上式中的 $p_\theta(x_{0:T})$ 与 q(x1:T∣x0)q(x_{1:T}\mid x_0)q(x1:T∣x0) 代入，并利用马尔可夫结构，可以将 ELBO 分解为若干项之和:
$$
\mathcal L_{\text{ELBO}}(x_0) = \mathbb E_{q(x_1\mid x_0)}[\log p_\theta(x_0\mid x_1)] -\mathrm{KL}\big(q(x_T\mid x_0)\ \|\ p(x_T)\big) -\sum_{t=2}^{T}\mathbb E_{q(x_t\mid x_0)}\Big[ \mathrm{KL}\big(q(x_{t-1}\mid x_t,x_0)\ \|\ p_\theta(x_{t-1}\mid x_t)\big) \Big].
$$
这一分解具有清晰的概率含义：

- **终止项（prior matching term）**: $\mathrm{KL}\big(q(x_T\mid x_0)\ \|\ p(x_T)\big)$

  衡量正向扩散到末端的分布是否与预设先验 $p(x_T)=\mathcal N(0,I)$对齐。直观上，它鼓励“扩散足够彻底”，使得 $x_T$ 近似标准高斯。

- **中间多步 KL 项（denoising matching term）**: $\sum_{t=2}^{T}\mathbb E_{q(x_t\mid x_0)}\Big[ \mathrm{KL}\big(q(x_{t-1}\mid x_t,x_0)\ \|\ p_\theta(x_{t-1}\mid x_t)\big) \Big]$

  逐步约束模型反向转移 pθ(xt−1∣xt)p_\theta(x_{t-1}\mid x_t)pθ(xt−1∣xt) 去逼近真实的后验反向分布 q(xt−1∣xt,x0)q(x_{t-1}\mid x_t,x_0)q(xt−1∣xt,x0)。这部分是“学会去噪”的核心。

- **重建项（reconstruction term）**:$\mathbb E_{q(x_1\mid x_0)}[\log p_\theta(x_0\mid x_1)]$

  描述最后一步从 $x_1$ 复原到 $x_0$ 的似然质量，可理解为对生成样本精细度的直接约束。

​	由此可见，DDPM 的训练本质上是在最大化 ELBO：一方面使末端噪声与先验匹配，另一方面在每个时间步都让反向链逼近真实的去噪后验，从而保证整个生成链条在概率意义上成立。



## 2.3 正向扩散过程建模（Forward Diffusion Process）

### 2.3.1 正向扩散过程的定义

​	在 DDPM 中，正向扩散过程被建模为一个**固定的马尔可夫随机过程**，其作用是将原始数据样本逐步转化为噪声样本。

![image-20251222150852107](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222150852107.png)

​	马尔可夫假设意味着当前状态 $x_t$ 仅依赖于前一时刻的状态 $x_{t-1}$，与更早的历史状态无关。在该假设下，正向扩散过程可以通过一系列条件高斯分布来刻画，其形式定义为
$$
q(x_t \mid x_{t-1}) = \mathcal{N}\!\left(\sqrt{1-\beta_t}\,x_{t-1},\ \beta_t I\right),
$$
其中 $\beta_t \in (0,1)$ 表示第 t 个时间步注入噪声的强度。该定义表明，在每一步扩散过程中，样本在保留部分原有信息的同时，被叠加了独立的高斯噪声，从而逐步偏离原始数据分布。从建模角度看，正向扩散过程是**人为设定且无需学习的**，其参数完全由预定义的噪声调度策略决定。这一特性使得正向过程在训练与采样阶段均可被精确控制，为后续反向生成过程的学习提供了稳定的参考分布。

### 2.3.2 噪声调度参数与物理意义

​	为了便于分析与实现，DDPM 中通常引入一组与 $\beta_t$ 相关的辅助参数。首先定义$\alpha_t = 1 - \beta_t$,并进一步定义其累积乘积$\bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s$.其中，$\alpha_t$ 描述了单步扩散中保留原有信号的比例，而 $\bar{\alpha}_t$ 则刻画了从初始时刻 0 到时间步 t 为止，原始数据 $x_0$ 在样本中所占的整体权重。

​	从直观上看，随着时间步 t 的增加，$\beta_t$ 所引入的噪声不断累积，使得 $\bar{\alpha}_t$ 逐渐减小，样本中来自 $x_0$ 的信息逐步被噪声淹没。当 ttt 足够大时，样本几乎完全由高斯噪声主导，其分布可以近似为标准正态分布。这一“逐步遗忘原始结构”的过程，正是扩散模型得名的物理直觉来源。

### 2.3.3 正向过程的封闭形式

​	基于上述参数定义，可以进一步推导正向扩散过程相对于初始数据 $x_0$ 的**封闭形式表达**。利用高斯分布在逐步线性变换下的封闭性，可以得到：
$$
q(x_t \mid x_0) = \mathcal{N}\!\left(\sqrt{\bar{\alpha}_t}\,x_0,\ (1-\bar{\alpha}_t)I\right).
$$
​	这一结果表明，在任意时间步 t，样本 $x_t$ 都可以被视为由原始数据 $x_0$ 与独立高斯噪声按比例线性叠加而成。进一步地，上式可以被重写为一步采样的形式：
$$
x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon,\quad \epsilon \sim \mathcal{N}(0,I).
$$
![image-20251222151404711](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222151404711.png)

------

## 2.4 反向生成过程与参数化建模（Reverse Process）

本节目标是将第 2.2 节中 ELBO 的中间项转化为一个**可训练的神经网络(Denoised Network)损失函数**。

### 2.4.1 反向生成过程

​	在扩散模型中，正向扩散过程通过逐步注入噪声将数据分布转化为近似高斯分布；与之对应，生成过程的核心任务在于**学习一个反向的去噪过程**，使模型能够从噪声样本逐步恢复出真实数据。为使反向生成过程在工程上可实现，DDPM 对反向条件分布作出合理的参数化假设，即将其建模为一个高斯分布：$p_\theta(x_{t-1}\mid x_t)$。

![The DDPM diagram. The upper row depicts the forward diffusion ...](https://www.researchgate.net/publication/370234882/figure/fig1/AS%3A11431281212194568%401702559206374/The-DDPM-diagram-The-upper-row-depicts-the-forward-diffusion-process-the-lower-row.tif?utm_source=chatgpt.com)

​	由于正向扩散为高斯马尔可夫过程，可证明理论反向分布亦为高斯，因此对模型分布作如下假设：
$$
p_\theta(x_{t-1}\mid x_t) = \mathcal N\!\big(\mu_\theta(x_t,t),\ \Sigma_\theta(x_t,t)\big).
$$
在DDPM原文中采用固定方差$\Sigma_\theta(x_t,t)=\sigma_t^2 I$，仅学习均值函数 $\mu_\theta(x_t,t)$

### 2.5.4 噪声预测网络

![image-20251222161847728](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222161847728.png)

​	DDPM 引入一个**共享参数的去噪神经网络（Denoiser Network）**，在所有时间步上复用，用以刻画反向生成过程的动态行为。该网络以当前带噪样本与时间步为输入
$$
(x_t,\ t)\ \longmapsto\ \text{Denoiser}(x_t,t) = \varepsilon_{\theta}(x_t,x)
$$
并通过输出对噪声或等价参数的预测，间接确定反向高斯分布的均值项。训练目标是用预测噪声近似前向扩散过程中的实际噪声。
$$
\varepsilon_{\theta}(x_t,x) \approx \varepsilon
$$


​	在下一小节，我们将说明该预测在数学上等价于对反向高斯分布 $p_\theta(x_{t-1}\mid x_t)$均值参数的建模，从而将最大化对数似然下界的问题转化为稳定的回归学习任务。

### 2.4.3 反向生成过程的训练目标

​	由 ELBO 分解可知，DDPM 的学习目标在于对每一时间步 t 最小化：
$$
\mathrm{KL}\big(q(x_{t-1}\mid x_t,x_0)\ \|\ p_\theta(x_{t-1}\mid x_t)\big).
$$
其中：理论反向分布$q(x_{t-1}\mid x_t,x_0)$是真实的一步去噪后验，因为正向扩散是线性高斯马尔可夫链，利用贝叶斯公式可计算出它满足的高斯分布
$$
q(x_{t-1}|x_t, x_0) = \frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_t|x_0)}，\\q(x_{t-1}\mid x_t,x_0)=\mathcal N\!\big(\tilde\mu_t(x_t,x_0),\ \tilde\beta_t I\big).\\
\boxed{ q(x_{t-1}\mid x_t,x_0) = \mathcal N\!\left( x_{t-1}; \ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\,x_0 +\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\,x_t, \ \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\, I \right) }
$$
可以看出， q的分布依赖 $x_0$，生成阶段不可用；模型反向分布$p_{\theta}(x_{t-1}\mid x_t)$是唯一需要学习的对象。使$p_{\theta}$的方差与q的方差一致，最小化KL散度等价于最小化均值之间的距离。

![image-20251222160851868](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222160851868.png)

在前向扩散加噪中，$x_t$与$x_0$的关系是确定的。
$$
x_t = \sqrt { \overline{\alpha}_t}x_0+\sqrt{1-\overline{\alpha}_t}\varepsilon
$$
 将上式代入14中的$q(x_{t-1}|x_t,x_0)$的均值表达式，可以得到
$$
\mu_q = \frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}}\varepsilon)
$$
$\mu_q$仅仅与$x_t$, $\varepsilon$有关，而$\varepsilon$是我们在Denoised Network中预测的噪声。因此DDPM的denoiser表面上是在去噪，实际上是在用噪声预测的方式间接学习反向高斯分布$p_{\theta}(x_{t-1}|x_{t})$的参数，从而最大化数据的对数似然下界。

------



## 2.6 DDPM 的训练与采样算法解析

DDPM 的核心实现可以概括为两套算法：

- **训练（Algorithm 1）**：随机选时间步 t，把真实样本 $x_0$ 加噪得到 $x_t$，训练Denoiser网络预测噪声 $\varepsilon$；
- **采样（Algorithm 2）**：从纯噪声 $x_T\sim \mathcal N(0,I)$ 出发，按 $t=T,T-1,\dots,1$ 逐步采样 $x_{t-1}\sim p_\theta(x_{t-1}\mid x_t)$。

### 2.6.1 训练算法（Algorithm 1）

<img src="/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222163524537.png" alt="image-20251222163524537" style="zoom:50%;" />

- 随机采样时间步 ( t )
- 构造带噪样本 ( $x_t$ )
- 噪声回归损失与梯度更新

<img src="/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222163834612.png" alt="image-20251222163834612" style="zoom:33%;" />

### 2.6.2 采样算法（Algorithm 2）

<img src="/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222163536895.png" alt="image-20251222163536895" style="zoom:50%;" />

在生成阶段，DDPM 采用反向扩散过程进行采样。该过程从各向同性高斯分布中初始化样本 $x_T \sim \mathcal{N}(0,I)$，并在时间步$t = T, T-1, \ldots, 1$ 上逐步执行反向更新。对于每一时间步，当前带噪样本 $x_t$ 与对应的时间编码 ttt 被输入去噪网络，网络输出对正向扩散噪声的估计 $\varepsilon_\theta(x_t,t)$。采用第t步的带噪样本$x_t$减去Denoiser预测出的噪声乘上权重系数$\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}}$，得到比$x_t$更加纯净的带噪图像$x_{t-1}$



![image-20251222163914919](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251222163914919.png)



## 第三章 算法设计：DDPM 的 PyTorch 复现

### 3.1 算法复现任务定义

本章围绕 **DDPM 的 PyTorch 级算法复现** 展开，目标是在不依赖现成扩散模型库的前提下，将 Ho et al.（2020）中提出的 DDPM 算法从数学形式完整转化为可运行的工程实现。

从算法设计角度看，DDPM 的训练与生成可被形式化为如下问题：

- **输入（训练阶段）**：
   来自真实数据分布 $p_{\text{data}}(x)$ 的样本 $x_0 \in \mathbb{R}^{C\times H\times W}$，以及随机采样的时间步 $t \in \{1,\dots,T\}$；
- **输出（训练阶段）**：
   去噪网络 $\varepsilon_\theta(x_t,t)$ 对正向扩散噪声 $\varepsilon$ 的预测结果；
- **输入（生成阶段）**：
   初始噪声样本 $x_T \sim \mathcal N(0,I)$；
- **输出（生成阶段）**：通过反向扩散过程逐步生成的样本 $x_0$，作为对真实数据分布的近似采样。

本项目的算法复现目标明确限定为 Ho et al. 在 *Denoising Diffusion Probabilistic Models*（2020）中提出的**原始 DDPM 框架**，具体包括：

- 正向扩散过程的定义与噪声调度方式；
- 基于 ELBO 推导得到的噪声预测训练目标；
- 以噪声预测参数化的反向生成过程；
- 原论文中给出的训练（Algorithm 1）与采样（Algorithm 2）流程。

​	在实现过程中，本项目**不直接调用**诸如 `diffusers`、`improved-diffusion` 等现成 DDPM 框架作为训练或生成的黑箱工具。



### 3.2 前向扩散算法的实现

#### 3.2.1 噪声调度策略的实现方式

​	根据第 2 章正向扩散定义 $q(x_t\mid x_{t-1})=\mathcal N(\sqrt{1-\beta_t}x_{t-1},\beta_t I)$，工程实现首先需要确定整条噪声日程 $\{\beta_t\}_{t=1}^T$。代码中通过 `get_beta_schedule` 生成 $\beta_t$:

```python
def get_beta_schedule(schedule, num_timesteps, beta_start, beta_end):
    # 根据调度类型生成长度为 T 的 beta 序列
    if schedule == "linear":
        betas = torch.linspace(beta_start, beta_end, num_timesteps)
    ...
    return betas
```

并在`GaussianDiffusion.__init__`中预计算 $\alpha_t=1-\beta_t$及 $\bar\alpha_t=\prod_{s=1}^t\alpha_s$，在训练和采样过程中即可通过时间步索引 t 直接获取 $\sqrt{\bar{\alpha}_t}$ 与 $\sqrt{1-\bar{\alpha}_t}$。

```python
self.betas = betas
self.alphas = 1.0 - self.betas
self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)

self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)
```



#### 3.2.2 正向扩散的一步采样实现

为了避免逐步迭代得到 $x_t$，实现中使用封闭形式
$$
q(x_t\mid x_0)=\mathcal N(\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I), \quad x_t=\sqrt{\bar\alpha_t}\,x_0+\sqrt{1-\bar\alpha_t}\,\varepsilon,\ \varepsilon\sim\mathcal N(0,I).
$$
对应代码在 `q_sample` 中以张量方式完成：对每个样本随机采样时间步 t，再通过索引提取 $\sqrt{\bar\alpha_t}$ 与 $\sqrt{1-\bar\alpha_t}$ 并广播到图像维度，从而实现一次运算构造 $x_t$。对应代码位于其中`GaussianDiffusion`类别下,   `extract` 函数的作用是根据批量中的时间步索引 ttt，从一维时间序列参数中取出对应的系数，并重塑为与输入图像 h

```python
def q_sample(self, x_start, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x_start)

    # 提取 sqrt(\bar{alpha}_t)，并 reshape 为 [B, 1, 1, 1] 以便广播
    sqrt_alphas_cumprod_t = extract(
        self.sqrt_alphas_cumprod, t, x_start.shape
    )
    sqrt_one_minus_alphas_cumprod_t = extract(
        self.sqrt_one_minus_alphas_cumprod, t, x_start.shape
    )

    # x_t = sqrt(\bar{alpha}_t) * x_0 + sqrt(1 - \bar{alpha}_t) * epsilon
    return sqrt_alphas_cumprod_t * x_start + \
           sqrt_one_minus_alphas_cumprod_t * noise

```



### 3.3 反向去噪与采样算法设计

#### 3.3.1 反向过程的参数化与去噪目标

在第 2.4 节中已经指出，DDPM 的反向生成过程通过最小化 ELBO 中的中间 KL 项
$$
\mathrm{KL}\big(q(x_{t-1}\mid x_t,x_0)\,\|\,p_\theta(x_{t-1}\mid x_t)\big)
$$
来学习反向条件分布。在工程实现中，反向分布被参数化为高斯形式
$$
p_\theta(x_{t-1}\mid x_t)=\mathcal N\!\big(\mu_\theta(x_t,t),\ \sigma_t^2 I\big),
$$
其中方差 $\sigma_t^2$ 采用预设策略固定，仅对均值函数 $\mu_\theta(x_t,t)$ 进行学习。根据第 2.4.4 节的推导结果，反向均值 $\mu_\theta(x_t,t)$ 可以通过预测正向扩散噪声$\varepsilon_\theta(x_t,t)$ 等价表示。因此，DDPM 将反向生成过程的学习目标统一转化为**噪声预测问题**，即学习函数$\varepsilon_\theta:\ (x_t,t)\ \mapsto\ \varepsilon$，从而间接确定反向高斯分布的均值参数。在代码实现中，这一参数化关系由 `p_mean_variance` 函数统一管理。该函数首先调用去噪网络得到噪声预测 $\varepsilon_\theta(x_t,t)$，再根据解析公式计算反向分布的均值与方差：

```python
def p_mean_variance(self, model, x, t):
    # 预测噪声 epsilon_theta(x_t, t)
    eps = model(x, t)

    # 根据噪声预测反推 x_0 的估计
    x_start = self.predict_xstart_from_eps(x, t, eps)

    # 计算反向高斯分布的均值和方差
    model_mean, _, model_log_variance = \
        self.q_posterior_mean_variance(x_start, x, t)

    return {
        "mean": model_mean,
        "log_variance": model_log_variance,
        "pred_xstart": x_start,
    }

```

​	其中，`predict_xstart_from_eps` 实现了由噪声预测反推 $x_0$ 的解析关系。根据正向扩散的封闭形式$x_t=\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\varepsilon$，可直接解得
$$
\hat x_0=\frac{1}{\sqrt{\bar\alpha_t}} \Big(x_t-\sqrt{1-\bar\alpha_t}\,\varepsilon_\theta(x_t,t)\Big).
$$
对应的代码实现如下：

```python
def predict_xstart_from_eps(self, x_t, t, eps):
    return (
        extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t
        - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps
    )

```

反之，在部分实现或分析中，也可由 x0x_0x0 的预测值反推出噪声项，其数学关系为



该关系在代码中由 `predict_eps_from_xstart` 给出：

#### 3.3.2 训练阶段：噪声预测损失的实现

在完成反向过程的参数化之后，DDPM 的训练目标可以被明确表述为： **在随机时间步 t 上，使模型预测的噪声 $\varepsilon_\theta(x_t,t)$ 尽可能接近真实注入噪声 $\varepsilon$**。

根据第 2.4 节的推导，ELBO 中所有与模型参数 $\theta$ 相关的项，最终可以被化简为一个关于噪声预测的均方误差目标：
$$
\mathcal L(\theta) = \mathbb E_{x_0\sim p_{\text{data}},\,t\sim\mathcal U(1,T),\,\varepsilon\sim\mathcal N(0,I)} \Big[ \|\varepsilon-\varepsilon_\theta(x_t,t)\|^2 \Big].
$$
该损失函数在数学上等价于对每一时间步上反向高斯分布与真实后验分布之间 KL 散度的加权和最小化，因此从概率意义上保证了模型对数据分布的最大似然逼近。这一训练目标由 `training_losses` 函数完成。其核心流程与上述期望形式严格一致，主要包含以下步骤：
 1）对每个样本随机采样时间步 t；
 2）根据第 3.2 节的方法构造对应的带噪样本 $x_t$；
 3）通过去噪网络预测噪声 $\varepsilon_\theta(x_t,t)$；
 4）计算预测噪声与真实噪声之间的均方误差。

```python
def training_losses(self, model, x_start, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x_start)

    # 构造 x_t = sqrt(\bar{alpha}_t)x_0 + sqrt(1-\bar{alpha}_t)epsilon
    x_t = self.q_sample(x_start=x_start, t=t, noise=noise)

    # 去噪网络预测噪声 epsilon_theta(x_t, t)
    model_output = model(x_t, t)

    # MSE 噪声回归损失
    loss = F.mse_loss(model_output, noise, reduction="none")
    loss = loss.mean(dim=list(range(1, len(loss.shape))))

    return loss

```

#### 3.3.3 采样阶段：反向一步更新的数值实现

​	在采样阶段，DDPM 从各向同性高斯分布$x_T \sim \mathcal N(0,I)$出发，通过反向扩散过程逐步生成样本。对于每一个时间步 $t=T,T-1,\ldots,1$，模型根据当前状态 $x_t$ 构造反向条件分布
$$
p_\theta(x_{t-1}\mid x_t) = \mathcal N\!\big(\mu_\theta(x_t,t),\ \sigma_t^2 I\big)
$$
​	并从该分布中采样得到下一步状态 $x_{t-1}$。根据第 3.3.1 节的参数化方式，反向均值 $\mu_\theta(x_t,t)$ 由噪声预测网络间接确定。具体而言，先利用模型输出 $\varepsilon_\theta(x_t,t)$ 反推出 $\hat x_0$，再结合正向扩散参数计算后验均值。该过程在工程中由3.3.1节中的 `p_mean_variance` 函数完成，并返回反向分布所需的均值与方差。在此基础上，单步反向采样的核心更新规则为：
$$
x_{t-1} = \mu_\theta(x_t,t) + \sigma_t z,\qquad z\sim\mathcal N(0,I),\\
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \Big( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\varepsilon_\theta(x_t,t) \Big) + \sigma_t z
$$
​	其中当 $t=1$ 时令 $z=0$，以避免在最终输出中重新引入噪声。对应的关键实现位于 `p_sample` 函数中：

```python
def p_sample(self, model, x, t):
    # 计算反向分布的均值与方差
    out = self.p_mean_variance(model, x, t)
    mean = out["mean"]
    log_variance = out["log_variance"]

    # 采样噪声项
    noise = torch.randn_like(x)
    nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))

    # x_{t-1} = mean + sigma_t * z （当 t>0）
    return mean + nonzero_mask * torch.exp(0.5 * log_variance) * noise
```

​	其中，`nonzero_mask` 用于在 $t=0$（或等价地 t=1t=1t=1 的最后一步）关闭噪声项，使采样过程在终止阶段变为确定性更新。该实现确保了采样过程严格遵循反向高斯分布的定义，同时兼顾数值稳定性。

### 3.4 噪声预测神经网络结构

​	DDPM采用U-Net 作为去噪网络的主体结构。U-Net 通过下采样路径逐步提取全局语义特征，并在上采样路径中恢复空间分辨率，同时借助跳跃连接保留局部细节信息。这种多尺度结构能够有效适应不同噪声强度下的去噪需求，是扩散模型中广泛采用的网络形式。

![image-20251223104938519](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251223104938519.png)



#### 3.4.1 ResNet模块

在 DDPM 中，去噪网络需要学习$\varepsilon_\theta(x_t,t)$，因此网络必须同时利用当前带噪样本 $x_t$ 的空间特征与时间步 $t$ 的条件信息。为此，本实现采用带 **timestep embedding 注入**的 ResNet 残差块作为 U-Net 的基本构件。其核心计算可以概括为“残差连接 + 两层卷积 + 时间条件加性调制”:
$$
h = \mathrm{Conv}_2(\mathrm{Dropout}(\phi(\mathrm{GN}( \mathrm{Conv}_1(\phi(\mathrm{GN}(x))) + W_t \phi(\mathrm{temb})) )))\\
\quad y = \mathrm{shortcut}(x) + h,
$$


其中 $\phi(\cdot)$ 为 SiLU(一种激活函数)，GN 为 GroupNorm，$\mathrm{temb}$ 为时间嵌入，$W_t$ 是时间投影层（Dense）。

![image-20251223111527822](/Users/yufei/course/算法设计与智能计算/project2/DDPM/Report/assets/image-20251223111527822.png)

下面的函数用到了一些特殊的模块将在`nn.py`内实现：

- `diffusion_nn.Conv2d`
- `diffusion_nn.Dense`
- `GroupNorm`
- `nonlinearity` = `GroupNorm` + `Silu`

```python
class ResnetBlock(nn.Module):
    """
    残差块（带时间步嵌入注入），对应 TF 版本 resnet_block()
    输入: x [B, C, H, W]，temb [B, temb_dim]
    输出: [B, out_ch, H, W]
    """
    def __init__(self, in_ch: int, out_ch: int = None, temb_dim: int = None,
                 conv_shortcut: bool = False, dropout: float = 0.0):
        super().__init__()
        self.in_ch = in_ch
        self.out_ch = out_ch if out_ch is not None else in_ch

        # 第 1 个归一化 + 卷积：GN -> SiLU -> Conv
        self.norm1 = GroupNorm32(in_ch)
        self.conv1 = diffusion_nn.Conv2d(in_ch, self.out_ch, filter_size=3)

        # 时间步嵌入投影：temb -> out_ch，并在空间维度上广播相加
        if temb_dim is not None:
            self.temb_proj = diffusion_nn.Dense(temb_dim, self.out_ch)

        # 第 2 个归一化 + Dropout + 卷积
        self.norm2 = GroupNorm32(self.out_ch)
        self.dropout = nn.Dropout(dropout)
        # init_scale=0 让残差分支初始接近 0，有利于稳定训练（常见 trick）
        self.conv2 = diffusion_nn.Conv2d(self.out_ch, self.out_ch, filter_size=3, init_scale=0.)

        # shortcut：当通道数变化时，用 Conv3x3 或 NIN(1x1) 对齐维度
        if in_ch != self.out_ch:
            if conv_shortcut:
                self.shortcut = diffusion_nn.Conv2d(in_ch, self.out_ch, filter_size=3)
            else:
                self.shortcut = diffusion_nn.NIN(in_ch, self.out_ch)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x: torch.Tensor, temb: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape

        # 主分支：GN -> SiLU -> Conv
        h = x
        h = nonlinearity(self.norm1(h))
        h = self.conv1(h)

        # 注入时间条件：h += proj(temb)（并在 H,W 上广播）
        if temb is not None and hasattr(self, 'temb_proj'):
            temb_out = self.temb_proj(nonlinearity(temb))  # [B, out_ch]
            h = h + temb_out[:, :, None, None]            # [B, out_ch, 1, 1] 广播到空间维

        # GN -> SiLU -> Dropout -> Conv
        h = nonlinearity(self.norm2(h))
        h = self.dropout(h)
        h = self.conv2(h)

        # 残差连接：shortcut(x) + h
        x = self.shortcut(x)
        assert x.shape == h.shape
        return x + h

```

#### 3.4.2 Upsampling 模块(上采样)

U-Net 的上采样路径用于逐步恢复空间分辨率，并与下采样路径的同尺度特征通过 skip-connection 融合。工程实现上，本项目采用 **nearest 插值上采样**，并可选用卷积进行特征整形：
$$
x \leftarrow \mathrm{Upsample}(x),\quad x \leftarrow \mathrm{Conv}(x)(可选)\
$$

```python
class Upsample(nn.Module):
    """
    上采样模块：将分辨率扩大 2 倍，可选卷积（with_conv=True 时）
    输入: [B, C, H, W] -> 输出: [B, C, 2H, 2W]
    """
    def __init__(self, channels: int, with_conv: bool):
        super().__init__()
        self.with_conv = with_conv
        # 可选：3x3 卷积用于平滑/整形上采样后的特征
        if with_conv:
            self.conv = diffusion_nn.Conv2d(channels, channels, filter_size=3, stride=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape
        # 最近邻插值，上采样到 2 倍空间分辨率
        x = F.interpolate(x, scale_factor=2, mode='nearest')
        if self.with_conv:
            x = self.conv(x)
        assert x.shape == (B, C, H * 2, W * 2)
        return x

```



#### 3.4.3 DownSampling 模块

下采样路径通过逐层降低分辨率扩大感受野，使网络在高噪声阶段也能捕获全局结构信息。本实现提供两种下采样方式：使用 stride=2 的卷积（更常用）或平均池化（更轻量）:
$$
x \leftarrow \mathrm{Downsample}(x)= \begin{cases} \mathrm{Conv}_{3\times 3,\ \text{stride}=2}(x), & \text{with\_conv=True}\\ \mathrm{AvgPool}_{2\times 2}(x), & \text{otherwise} \end{cases}
$$

```python
class Downsample(nn.Module):
    """
    下采样模块：将分辨率缩小 2 倍，可选卷积（with_conv=True 时）
    输入: [B, C, H, W] -> 输出: [B, C, H//2, W//2]
    """
    def __init__(self, channels: int, with_conv: bool):
        super().__init__()
        self.with_conv = with_conv
        if with_conv:
            # stride=2 的卷积实现下采样
            self.conv = diffusion_nn.Conv2d(channels, channels, filter_size=3, stride=2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape
        if self.with_conv:
            x = self.conv(x)
        else:
            # 备选：平均池化下采样
            x = F.avg_pool2d(x, 2, 2)
        assert x.shape == (B, C, H // 2, W // 2)
        return x
```



#### 3.4.4 Timestep Embeding 模块

​	扩散模型的去噪行为强烈依赖时间步 $t$：当 $t$ 大时噪声强、去噪需要更“粗”的结构恢复；当 ttt 小时噪声弱、去噪需要更多细节修复。为将时间信息显式注入网络，代码首先用 `get_timestep_embedding(t, ch)` 将离散时间步映射为向量，再经两层全连接（MLP）得到最终条件向量 $\mathrm{temb}$：
$$
\mathrm{temb}=\mathrm{MLP}(\mathrm{Embed}(t)) = W_2\,\phi(W_1\,\mathrm{Embed}(t))
$$
并在每个 ResNetBlock 中以“加性偏置”的形式注入：
$$
h \leftarrow h + W_t\,\phi(\mathrm{temb}).
$$


```python
class UNet(nn.Module):
    def __init__(...):
        ...
        temb_dim = ch * 4

        # ===== 时间步嵌入的两层 MLP（t -> temb）=====
        self.temb_dense0 = diffusion_nn.Dense(ch, temb_dim)
        self.temb_dense1 = diffusion_nn.Dense(temb_dim, temb_dim)
        ...

    def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:
        B, C, H, W = x.shape

        # ===== 1) 计算 timestep embedding =====
        # get_timestep_embedding 通常为正弦/余弦位置编码，将 t 映射到 [B, ch]
        temb = diffusion_nn.get_timestep_embedding(t, self.ch)  # [B, ch]
        temb = self.temb_dense0(temb)                           # [B, 4ch]
        temb = nonlinearity(temb)                               # SiLU
        temb = self.temb_dense1(temb)                           # [B, 4ch]
        assert temb.shape == (B, self.ch * 4)

        # temb 将被传入每个 ResnetBlock，用于条件化去噪
        ...

```



#### 3.4.5 顶层Unet模块

​	UNet 的顶层模块由一个统一的 `UNet` 类封装，其前向传播函数 `forward` 明确规定了数据与条件信息在网络中的流动方式。整体上，该结构可被概括为**带时间条件的多尺度编码–解码网络（Encoder–Decoder with skip connections）**。

​	在顶层 UNet 中，时间步 t 首先被映射为连续向量表示，并通过两层全连接网络生成高维时间嵌入 `temb`，该向量随后被传入网络中所有 ResNetBlock。

```python
temb = diffusion_nn.get_timestep_embedding(t, self.ch)
temb = self.temb_dense0(temb)
temb = nonlinearity(temb)
temb = self.temb_dense1(temb)
```

UNet 的下采样路径由若干级 ResNetBlock 与 Downsample 组合构成，用于逐步降低空间分辨率、扩大感受野，并提取全局结构特征。每一级下采样的输出都会被缓存，用于后续的跳跃连接。

```python
h = self.conv_in(x)
hs = [h]

for i_level in range(self.num_resolutions):
    for i_block in range(self.num_res_blocks):
        h = self.down_blocks[block_idx](h, temb)
        hs.append(h)
    if i_level != self.num_resolutions - 1:
        h = self.down_samples[sample_idx](h)
        hs.append(h)
```

​	在网络的最底层，UNet 设置了一个由 ResNetBlock 与自注意力模块组成的中间结构,此时特征图分辨率最低、感受野最大，该层在整体网络中起到“语义瓶颈”的作用，用于在强噪声条件下整合全局依赖关系，为后续的逐步细化提供稳定基础。

```python
h = self.mid_block1(h, temb)
h = self.mid_attn(h)
h = self.mid_block2(h, temb)
```

​	上采样路径在结构上与下采样路径对称，其核心操作包括：
 1）将当前特征与对应尺度的缓存特征进行拼接（skip connection）；
 2）通过 ResNetBlock 在当前时间条件下融合多尺度信息；
 3）通过 Upsample 恢复空间分辨率。

```python
for i_level in reversed(range(self.num_resolutions)):
    for i_block in range(self.num_res_blocks + 1):
        h = torch.cat([h, hs.pop()], dim=1)
        h = self.up_blocks[block_idx](h, temb)
    if i_level != 0:
        h = self.up_samples[sample_idx](h)
```

​	在网络末端，UNet 通过归一化、非线性激活与卷积层，将特征映射回与输入同形状的输出张量：

```python
h = nonlinearity(self.norm_out(h))
h = self.conv_out(h)
```

​	该输出被直接解释为噪声预测 $\varepsilon_\theta(x_t,t)$，并用于第 3.3 节中构造反向高斯分布的均值参数。由此，UNet 顶层模块在整个 DDPM 框架中承担了**反向生成过程参数化核心**的角色。



## 第四章 算法复杂度与性能分析

（Complexity and Performance Analysis）

### 4.1 时间复杂度分析

​	DDPM 的整体时间复杂度由两部分共同决定：一是噪声预测网络（通常为 U-Net 结构）的单次前向与反向计算代价，二是扩散或反扩散过程中网络被调用的次数。Ho 等人在原始 DDPM 工作中给出了标准的训练与采样算法，这为复杂度分析提供了明确的算法基础（Ho et al., 2020, *Denoising Diffusion Probabilistic Models*）。

​	在训练阶段，DDPM 并不会对所有时间步进行完整展开。根据 Ho 等人提出的训练策略，每一次参数更新仅对单一样本随机采样一个时间步 $t$，并构造对应的加噪样本 $x_t$，然后通过一次噪声预测网络 $\varepsilon_{\theta}(x_t, t)$ 的前向传播计算损失，并进行反向传播更新参数。因此，训练阶段的时间复杂度不与扩散总步数 T 成线性关系，而主要取决于训练迭代次数、批大小以及网络本身的计算代价。

​	若记噪声预测网络一次前向传播的计算量为 $F$（可用 FLOPs 或等价时间成本衡量），反向传播的代价通常为前向传播的常数倍，记为 $\kappa F$（该比例依赖于具体算子与实现方式，在常见卷积网络中通常为经验量级估计）。设训练总迭代次数为 N_iter，batch size 为 B，则训练阶段的总体时间复杂度可近似表示为 
$$
\mathrm{N_iter}\cdot B \cdot (1+\kappa)F
$$
该表达式强调，训练复杂度的主要来源是网络前后向计算，而非扩散时间步数本身，这一点是 DDPM 与直觉认知中“多步扩散必然导致训练极慢”的关键区别。

​	进一步考察单次前向传播计算量 F 的来源，可以从 U-Net 结构进行分析。U-Net 由多尺度卷积模块构成，包含逐级下采样与上采样路径，并通过跳跃连接融合多尺度特征（Ronneberger et al., 2015, *U-Net: Convolutional Networks for Biomedical Image Segmentation*）。对于标准二维卷积层，其计算复杂度量级可表示为 
$$
\mathrm{F_conv} = O(HW\cdot k^2\cdot \mathrm{C_in} \cdot \mathrm{C_out})
$$
其中 H、W 为特征图空间分辨率，k 为卷积核尺寸，$\mathrm{C_in}$ 与 $\mathrm{C_out}$ 分别为输入与输出通道数。将网络中不同尺度层的卷积计算量加总，即可得到整体前向传播复杂度 F。需要指出的是，在 DDPM 的改进版本（如 IDDPM）中，部分分辨率层会引入自注意力机制，此时注意力模块的计算复杂度在对应尺度上近似为 $O((HW)²\cdot d)$，从而显著改变单次前向计算的成本结构（Nichol & Dhariwal, 2021, *Improved Denoising Diffusion Probabilistic Models*）。

​	在采样阶段，DDPM 的时间复杂度特征则与训练阶段明显不同。根据 Ho 等人提出的反向扩散采样算法，模型从高斯噪声 $x_T\sim \mathcal N (0,I)$ 出发，依次对时间步 $t=T,T−1,…,1$ 进行迭代，每一步均需调用一次噪声预测网络 $\varepsilon_{\theta}(x_t, t)$，并通过简单的线性变换得到 $x_{t−1}$。因此，在不考虑反向传播的情况下，采样阶段的总体时间复杂度近似为 
$$
\text{Sample-Time} \approx S \cdot F
$$
其中 S 表示实际采用的采样步数。在原始 DDPM 中通常取 $S=T$，这也是 DDPM 生成速度较慢的主要原因。

​	近年来的研究表明，采样复杂度不仅取决于模型结构，还与具体采样策略密切相关。例如，DDIM 通过减少采样步数，把 S 从 1000 降到 50/100，在一定程度上实现了生成速度与样本质量之间的折中（Song et al., 2021）。在连续时间扩散模型的统一框架下，Song 等人进一步将反向扩散过程表示为随机微分方程或其对应的概率流常微分方程，采样过程可视为数值积分问题，其计算成本由网络函数评估次数（Number of Function Evaluations, NFE）决定（Song et al., 2021, *Score-Based Generative Modeling through Stochastic Differential Equations*）。需要指出的是，ODE/SDE 采样在某些设置下可以显著减少网络调用次数，但其效率高度依赖于误差容忍度、数值求解器以及模型光滑性，并非在所有情形下都优于离散采样。

### 4.2 空间复杂度分析

​	DDPM 的空间复杂度主要来源于三部分：模型参数及其优化器状态、训练过程中保存的中间激活，以及扩散或反扩散过程中所需的状态变量。不同阶段对显存的需求差异较大，尤其体现在训练与采样阶段之间。

​	**首先，模型参数本身占用的存储空间与参数量 P 成正比，为 $O(P)$。** 在实际训练中，常用的 Adam 等自适应优化器还需要为每个参数维护一阶与二阶动量变量，从而引入额外约 $2P$ 规模的存储开销；若采用指数滑动平均（EMA）等技术，还需额外保存一份参数副本。因此，仅参数及优化器状态相关的显存占用通常为 $O(P)$ 的若干倍，这一部分在训练与采样阶段均不可避免。

​	其次，训练阶段的显存瓶颈通常来自**中间激活的存储**。由于反向传播需要保留前向计算过程中各层的激活值，训练时的显存需求随网络深度、特征图空间分辨率以及批大小共同增长。对于多尺度 U-Net 结构，其训练阶段的激活存储需求可粗略表示为
$$
\text{Train-Memory} \approx O(P) + O(\sum_l B H_l W_l C_l)
$$
其中 $H_l$、$W_l$、$C_l$ 分别表示第 $l$ 层特征图的空间尺寸与通道数。由于高分辨率层的特征图体积最大，这些层往往成为显存占用的主要来源。实际工程中，激活检查点（activation checkpointing）等技术可在一定程度上降低显存需求，但会以增加计算量为代价。

​	相比之下，采样或推理阶段的空间复杂度显著降低。由于**无需进行反向传播**，网络不需要保存全部中间激活，显存主要由模型参数和当前层激活所占据，其规模近似为
$$
\text{Sample-Memory} \approx O(P) + O(\max_l H_l W_l C_l)
$$
这一特性使得在相同硬件条件下，DDPM 的采样阶段通常可以支持更大的 batch size 或更高分辨率的输入。

​	最后，需要特别指出的是，尽管 DDPM 的采样过程涉及多个时间步，但**其空间复杂度并不会随采样步数线性增长**。反向扩散过程本质上是一个马尔可夫链，仅需保存当前状态 x_t 以及少量辅助变量，而无需存储完整的状态序列 {x_1,…,x_T}。因此，扩散链本身的状态存储开销为 $O(D)$，其中 D 表示单个样本的维度，与采样步数 S 无关。

​	综上所述，DDPM 的空间复杂度在训练阶段主要受模型参数规模与中间激活存储主导，而在采样阶段则显著降低，表现出“训练显存占用高、采样显存占用相对可控”的典型特征。这一差异也是扩散模型在实际部署与推理阶段的重要工程优势之一。



## 第五章 实验设计与结果分析

（Experiments and Results）

### 5.1 实验环境与数据集

- CIFAR-10 数据集介绍
- 硬件与软件环境说明

### 5.2 Baseline 实验：PyTorch 复现的 DDPM（核心）

- 实验目的：验证算法复现的正确性
- 训练过程与损失变化
- 生成样本可视化结果
- 生成效果的定性分析

### 5.3 拓展实验：OpenAI Improved DDPM

- 引入 Improved DDPM 的动机
- 改进点简要说明（如噪声建模、采样方式）
- 实验结果展示

### 5.4 Baseline 与 Improved DDPM 的对比分析

- 生成样本质量对比
- 训练稳定性对比
- 算法改进带来的性能变化分析

------

## 第六章 讨论：算法复现中的理论与工程问题

（Discussion）

### 6.1 理论公式与工程实现的差异

- 数值稳定性问题
- 参数设置对结果的影响

### 6.2 框架差异带来的实现影响

- PyTorch 与 TensorFlow 的实现差异
- 自动求导与张量操作的不同处理方式

### 6.3 项目局限性与改进方向

- 采样速度问题
- 可能的拓展方向（DDIM、Score-based 方法）

------

## 第七章 小组分工与贡献说明

（Teamwork and Contribution）

### 7.1 小组成员信息

- 姓名
- 学号

### 7.2 分工情况说明

- 理论推导
- 算法复现
- 实验分析
- 报告与展示

### 7.3 贡献比例说明

- 明确贡献比例
- 成绩计算方式说明

------

## 第八章 总结与展望（Conclusion）

- 本项目完成情况总结
- 主要收获与经验
- 后续可深入研究的方向

------

## 参考文献（References）

- DDPM 原论文（Ho et al., 2020）
- OpenAI Improved DDPM 相关论文与代码
- 其他参考资料（明确标注来源）
